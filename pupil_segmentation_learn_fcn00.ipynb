{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nakazawa_atsushi/anaconda3/envs/py3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/nakazawa_atsushi/anaconda3/envs/py3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "#   LEARN FCN00\n",
    "#\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Concatenate, AveragePooling2D\n",
    "from keras.layers import merge\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.preprocessing.image import list_pictures, array_to_img\n",
    "\n",
    "from image_ext import list_pictures_in_multidir, load_imgs_asarray, img_dice_coeff\n",
    "from fname_func import load_fnames, make_fnames\n",
    "\n",
    "# MAXPOOLING\n",
    "from create_fcn import create_fcn01, create_fcn00\n",
    "# AVERAGE POOLING\n",
    "#from create_fcn_avpool import create_fcn01,create_fcn00\n",
    "\n",
    "np.random.seed(2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true = K.flatten(y_true)\n",
    "    y_pred = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true * y_pred)\n",
    "    return (2.*intersection + 1) / (K.sum(y_true) + K.sum(y_pred) + 1)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model fcn00...\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#  MAIN STARTS FROM HERE\n",
    "#\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    target_size = (224, 224)\n",
    "    dpath_this = './'\n",
    "    # dname_checkpoints = 'checkpoints_fcn00_avpool.augumented'\n",
    "    dname_checkpoints = 'checkpoints_fcn00.augumented'\n",
    "    # dname_checkpoints_fcn01 = 'checkpoints_fcn01_avpool'\n",
    "    dname_checkpoints_fcn01 = 'checkpoints_fcn01'\n",
    "    dname_outputs = 'outputs'\n",
    "    fname_architecture = 'architecture.json'\n",
    "    fname_weights = \"model_weights_{epoch:02d}.h5\"\n",
    "    fname_stats = 'stats01.npz'\n",
    "    dim_ordering = 'channels_first'\n",
    "    fname_history = \"history.pkl\"\n",
    "\n",
    "    # definision of mode, LEARN or TEST or SHOW_HISTORY\n",
    "    #mode = \"LEARN\"\n",
    "    #mode = \"SHOW_HISTORY\"\n",
    "    #mode = \"TEST\"\n",
    "\n",
    "    # モデルを作成\n",
    "    print('creating model fcn00...')\n",
    "    model_fcn00 = create_fcn00(target_size)\n",
    "    \n",
    "    if os.path.exists(dname_checkpoints) == 0:\n",
    "        os.mkdir(dname_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading training data\n",
      "reading traking gt data\n",
      "reading validation data\n",
      "==> 7800 training images loaded\n",
      "==> 7800 training masks loaded\n",
      "==> 1196 validation images loaded\n",
      "==> 1196 validation masks loaded\n",
      "computing mean and standard deviation...\n",
      "==> mean: [125.60018   90.205666  77.57043 ]\n",
      "==> std : [61.01421  47.890713 51.63054 ]\n"
     ]
    }
   ],
   "source": [
    "    #\n",
    "    #   LEARNING MODE\n",
    "    #\n",
    "    # Read Learning Data\n",
    "    #    fnames = load_fnames('data/list_train_01.txt')\n",
    "    #    [fpaths_xs_train,fpaths_ys_train] = make_fnames(fnames,'data/img','data/mask','OperatorA_')\n",
    "    #    fnames = load_fnames('data.nnlab/list_train_01.txt')\n",
    "    #    fnames = load_fnames('data/list_train_01.txt')\n",
    "    fnames = load_fnames('data_augumented/list_train_01.txt')\n",
    "    #    [fpaths_xs_train,fpaths_ys_train] = make_fnames(fnames,'data.nnlab/image','data.nnlab/gt','')\n",
    "    [fpaths_xs_train,fpaths_ys_train] = make_fnames(fnames,'data_augumented/img','data_augumented/mask','')\n",
    "\n",
    "    print('reading training data')\n",
    "    X_train = load_imgs_asarray(fpaths_xs_train, grayscale=False, target_size=target_size,\n",
    "                                dim_ordering=dim_ordering)\n",
    "    print('reading traking gt data')\n",
    "    Y_train = load_imgs_asarray(fpaths_ys_train, grayscale=True, target_size=target_size,\n",
    "                                dim_ordering=dim_ordering) \n",
    "\n",
    "    # Read Validation Data\n",
    "    #    fnames = load_fnames('data/list_valid_01.txt')\n",
    "    #    [fpaths_xs_valid,fpaths_ys_valid] = make_fnames(fnames,'data/img','data/mask','OperatorA_')\n",
    "    fnames = load_fnames('data_augumented/list_valid_01.txt')\n",
    "    [fpaths_xs_valid,fpaths_ys_valid] = make_fnames(fnames,'data_augumented/img','data_augumented/mask','')\n",
    "    \n",
    "    print('reading validation data')\n",
    "    X_valid = load_imgs_asarray(fpaths_xs_valid, grayscale=False, target_size=target_size,\n",
    "                                dim_ordering=dim_ordering)\n",
    "    Y_valid = load_imgs_asarray(fpaths_ys_valid, grayscale=True, target_size=target_size,\n",
    "                                dim_ordering=dim_ordering)     \n",
    "\n",
    "    print('==> ' + str(len(X_train)) + ' training images loaded')\n",
    "    print('==> ' + str(len(Y_train)) + ' training masks loaded')\n",
    "    print('==> ' + str(len(X_valid)) + ' validation images loaded')\n",
    "    print('==> ' + str(len(Y_valid)) + ' validation masks loaded')\n",
    "\n",
    "    # 前処理\n",
    "    print('computing mean and standard deviation...')\n",
    "    mean = np.mean(X_train, axis=(0, 2, 3))\n",
    "    std = np.std(X_train, axis=(0, 2, 3))\n",
    "    print('==> mean: ' + str(mean))\n",
    "    print('==> std : ' + str(std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving mean and standard deviation to stats01.npz...\n",
      "==> done\n",
      "globally normalizing data...\n",
      "==> done\n"
     ]
    }
   ],
   "source": [
    "    print('saving mean and standard deviation to ' + fname_stats + '...')\n",
    "    stats = {'mean': mean, 'std': std}\n",
    "    np.savez(dname_checkpoints + '/' + fname_stats, **stats)\n",
    "    print('==> done')\n",
    "\n",
    "    print('globally normalizing data...')\n",
    "    for i in range(3):\n",
    "        X_train[:, i] = (X_train[:, i] - mean[i]) / std[i]\n",
    "        X_valid[:, i] = (X_valid[:, i] - mean[i]) / std[i]\n",
    "    Y_train /= 255\n",
    "    Y_valid /= 255\n",
    "    print('==> done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copying layer weights\n",
      "conv1_1\n",
      "conv1_2\n",
      "conv2_1\n",
      "conv2_2\n",
      "up1_1\n",
      "up1_2\n",
      "up2_1\n",
      "up2_2\n",
      "conv_fin\n"
     ]
    }
   ],
   "source": [
    "    init_from_fcn01 = 1\n",
    "    \n",
    "    if init_from_fcn01 == 1:\n",
    "        # モデルに学習済のfcn01 Weightをロードする\n",
    "        model_fcn01 = create_fcn01(target_size)        \n",
    "        epoch = 100\n",
    "        fname_weights = 'model_weights_%02d.h5'%(epoch)\n",
    "        fpath_weights_fcn01 = os.path.join(dname_checkpoints_fcn01, fname_weights)\n",
    "        model_fcn01.load_weights(fpath_weights_fcn01)\n",
    "        #print('==> done')\n",
    "\n",
    "        # load weights from Learned U-NET\n",
    "        layer_names = ['conv1_1','conv1_2','conv2_1','conv2_2','conv3_1','conv3_2',\n",
    "                       'conv4_1','conv4_2','conv5_1', 'conv5_2',\n",
    "                    'up1_1', 'up1_2', 'up2_1', 'up2_2', 'up3_1', 'up3_2', 'up4_1', \n",
    "                       'up4_2', 'conv_fin']\n",
    "        layer_names = ['conv1_1','conv1_2','conv2_1','conv2_2',\n",
    "                    'up1_1', 'up1_2', 'up2_1', 'up2_2', 'conv_fin']\n",
    "\n",
    "        print('copying layer weights')\n",
    "        for name in layer_names:\n",
    "            print(name)\n",
    "            model_fcn00.get_layer(name).set_weights(model_fcn01.get_layer(name).get_weights())\n",
    "            model_fcn00.get_layer(name).trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # 損失関数，最適化手法を定義\n",
    "    adam = Adam(lr=1e-5)\n",
    "    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.95, nesterov=True)\n",
    "    #rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "    model_fcn00.compile(optimizer=adam, loss=dice_coef_loss, metrics=[dice_coef])\n",
    "\n",
    "    # 構造・重みを保存するディレクトリーの有無を確認\n",
    "    dpath_checkpoints = os.path.join(dpath_this, dname_checkpoints)\n",
    "    if not os.path.isdir(dpath_checkpoints):\n",
    "        os.mkdir(dpath_checkpoints)\n",
    "\n",
    "    # 重みを保存するためのオブジェクトを用意\n",
    "    fname_weights = \"model_weights_{epoch:02d}.h5\"\n",
    "    fpath_weights = os.path.join(dpath_checkpoints, fname_weights)\n",
    "    checkpointer = ModelCheckpoint(filepath=fpath_weights, save_best_only=False)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n"
     ]
    }
   ],
   "source": [
    "    # トレーニングを開始\n",
    "    print('start training...')\n",
    "    history = model_fcn00.fit(X_train[:,:,:,:], Y_train[:,:,:,:], batch_size=64, epochs=200, verbose=1,\n",
    "                  shuffle=True, validation_data=(X_valid, Y_valid), callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Save History\n",
    "    f = open(dname_checkpoints + '/' + fname_history,'wb')\n",
    "    pickle.dump(history.history,f)\n",
    "    f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #\n",
    "    #   Show History\n",
    "    #\n",
    "\n",
    "    # load pickle\n",
    "    print(dname_checkpoints + '/' + fname_history)\n",
    "    history = pickle.load(open(dname_checkpoints + '/' + fname_history, 'rb'))\n",
    "\n",
    "    for k in history.keys():\n",
    "        plt.plot(history[k])\n",
    "        plt.title(k)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
