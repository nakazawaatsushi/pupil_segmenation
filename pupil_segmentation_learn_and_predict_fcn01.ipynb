{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "#   LEARN FCN01 from FCN02\n",
    "#\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Concatenate\n",
    "from keras.layers import merge\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.preprocessing.image import list_pictures, array_to_img\n",
    "\n",
    "from image_ext import list_pictures_in_multidir, load_imgs_asarray, img_dice_coeff\n",
    "from create_fcn import create_fcn01, create_fcn02, create_pupil_net\n",
    "\n",
    "np.random.seed(2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true = K.flatten(y_true)\n",
    "    y_pred = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true * y_pred)\n",
    "    return (2.*intersection + 1) / (K.sum(y_true) + K.sum(y_pred) + 1)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fnames(paths):\n",
    "    f = open(paths)\n",
    "    data1 = f.read()\n",
    "    f.close()\n",
    "    lines = data1.split('\\n')\n",
    "    #print(len(lines))\n",
    "    # 最終行は空行なので消す\n",
    "    del(lines[len(lines)-1])\n",
    "    #print(len(lines))\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fnames(fnames,fpath,fpath_mask,mask_ext):\n",
    "    fnames_img = [];\n",
    "    fnames_mask= [];\n",
    "    \n",
    "    for i in range(len(fnames)):\n",
    "        fnames_img.append(fpath + '/' + fnames[i]);\n",
    "        fnames_mask.append(fpath_mask + '/' + mask_ext + fnames[i]);\n",
    "        \n",
    "    return [fnames_img,fnames_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model fcn01 and fcn02...\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#  MAIN STARTS FROM HERE\n",
    "#\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    target_size = (224, 224)\n",
    "    dpath_this = './'\n",
    "    dname_checkpoints = 'checkpoints_fcn01'\n",
    "    dname_outputs = 'outputs'\n",
    "    fname_architecture = 'architecture.json'\n",
    "    fname_weights = \"model_weights_{epoch:02d}.h5\"\n",
    "    fname_stats = 'stats01.npz'\n",
    "    dim_ordering = 'channels_first'\n",
    "    fname_history = \"history.pkl\"\n",
    "\n",
    "    # definision of mode, LEARN or TEST or SHOW_HISTORY\n",
    "    mode = \"LEARN\"\n",
    "    #mode = \"SHOW_HISTORY\"\n",
    "    #mode = \"TEST\"\n",
    "\n",
    "    # モデルを作成\n",
    "    print('creating model fcn01 and fcn02...')\n",
    "    model_fcn02 = create_fcn02(target_size)\n",
    "    model_fcn01 = create_fcn01(target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 1452 training images loaded\n",
      "==> 1452 training masks loaded\n",
      "==> 527 validation images loaded\n",
      "==> 527 validation masks loaded\n",
      "computing mean and standard deviation...\n",
      "==> mean: [ 130.65464783   91.26850128   76.63642883]\n",
      "==> std : [ 55.28170013  43.99096298  43.11348343]\n",
      "saving mean and standard deviation to stats01.npz...\n",
      "==> done\n",
      "globally normalizing data...\n",
      "==> done\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#   LEARNING MODE\n",
    "#\n",
    "if mode == \"LEARN\":\n",
    "    # Read Learning Data\n",
    "    fnames = load_fnames('data/list_train_01.txt')\n",
    "    [fpaths_xs_train,fpaths_ys_train] = make_fnames(fnames,'data/img','data/mask','OperatorA_')\n",
    "\n",
    "    X_train = load_imgs_asarray(fpaths_xs_train, grayscale=False, target_size=target_size,\n",
    "                                dim_ordering=dim_ordering)\n",
    "    Y_train = load_imgs_asarray(fpaths_ys_train, grayscale=True, target_size=target_size,\n",
    "                                dim_ordering=dim_ordering) \n",
    "\n",
    "    # Read Validation Data\n",
    "    fnames = load_fnames('data/list_valid_01.txt')\n",
    "    [fpaths_xs_valid,fpaths_ys_valid] = make_fnames(fnames,'data/img','data/mask','OperatorA_')\n",
    "\n",
    "    X_valid = load_imgs_asarray(fpaths_xs_valid, grayscale=False, target_size=target_size,\n",
    "                                dim_ordering=dim_ordering)\n",
    "    Y_valid = load_imgs_asarray(fpaths_ys_valid, grayscale=True, target_size=target_size,\n",
    "                                dim_ordering=dim_ordering)     \n",
    "\n",
    "    print('==> ' + str(len(X_train)) + ' training images loaded')\n",
    "    print('==> ' + str(len(Y_train)) + ' training masks loaded')\n",
    "    print('==> ' + str(len(X_valid)) + ' validation images loaded')\n",
    "    print('==> ' + str(len(Y_valid)) + ' validation masks loaded')\n",
    "\n",
    "    # 前処理\n",
    "    print('computing mean and standard deviation...')\n",
    "    mean = np.mean(X_train, axis=(0, 2, 3))\n",
    "    std = np.std(X_train, axis=(0, 2, 3))\n",
    "    print('==> mean: ' + str(mean))\n",
    "    print('==> std : ' + str(std))\n",
    "\n",
    "    print('saving mean and standard deviation to ' + fname_stats + '...')\n",
    "    stats = {'mean': mean, 'std': std}\n",
    "    np.savez(dname_checkpoints + '/' + fname_stats, **stats)\n",
    "    print('==> done')\n",
    "\n",
    "    print('globally normalizing data...')\n",
    "    for i in range(3):\n",
    "        X_train[:, i] = (X_train[:, i] - mean[i]) / std[i]\n",
    "        X_valid[:, i] = (X_valid[:, i] - mean[i]) / std[i]\n",
    "    Y_train /= 255\n",
    "    Y_valid /= 255\n",
    "    print('==> done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # モデルに学習済のU-NET Weightをロードする\n",
    "    epoch = 300\n",
    "    fname_weights = 'model_weights_%02d.h5'%(epoch)\n",
    "    fpath_weights_fcn02 = os.path.join(dname_checkpoints_fcn02, fname_weights)\n",
    "    model.load_weights(fpath_weights_fcn02)\n",
    "    print('==> done')\n",
    "\n",
    "    # load weights from Learned U-NET\n",
    "    layer_names = ['conv1_1','conv1_2','conv2_1','conv2_2','conv3_1','conv3_2','conv4_1','conv4_2',\n",
    "                'up1_1', 'up1_2', 'up2_1', 'up2_2', 'up3_1', 'up3_2']\n",
    "    \n",
    "    print('copying layer weights')\n",
    "    for name in layer_names:\n",
    "        print(name)\n",
    "        model_pupil_net.get_layer(name).set_weights(model.get_layer(name).get_weights())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 損失関数，最適化手法を定義\n",
    "    adam = Adam(lr=1e-5)\n",
    "    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.1, nesterov=True)\n",
    "    #rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(optimizer=adam, loss=dice_coef_loss, metrics=[dice_coef])\n",
    "\n",
    "    # 構造・重みを保存するディレクトリーの有無を確認\n",
    "    dpath_checkpoints = os.path.join(dpath_this, dname_checkpoints)\n",
    "    if not os.path.isdir(dpath_checkpoints):\n",
    "        os.mkdir(dpath_checkpoints)\n",
    "    # モデルの構造を保存\n",
    "    json_string = model.to_json()\n",
    "    fpath_architecture = os.path.join(dpath_checkpoints, fname_architecture)\n",
    "    with open(fpath_architecture, \"w\", encoding=\"utf8\") as f:\n",
    "        f.write(json_string)\n",
    "    # 重みを保存するためのオブジェクトを用意\n",
    "    fpath_weights = os.path.join(dpath_checkpoints, fname_weights)\n",
    "    checkpointer = ModelCheckpoint(filepath=fpath_weights, save_best_only=False)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-be08c15cbcb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# トレーニングを開始\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'start training...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m history = model.fit(X_train, Y_train, batch_size=32, epochs=100, verbose=1,\n\u001b[0m\u001b[0;32m      4\u001b[0m               shuffle=True, validation_data=(X_valid, Y_valid), callbacks=[checkpointer])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "    # トレーニングを開始\n",
    "    print('start training...')\n",
    "    history = model.fit(X_train, Y_train, batch_size=32, epochs=100, verbose=1,\n",
    "                  shuffle=True, validation_data=(X_valid, Y_valid), callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-dadc4c67b0f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Save History\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdname_checkpoints\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfname_history\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "    # Save History\n",
    "    f = open(dname_checkpoints + '/' + fname_history,'wb')\n",
    "    pickle.dump(history.history,f)\n",
    "    f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> done\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#  TEST MODE\n",
    "#\n",
    "if mode == \"TEST\":\n",
    "    # Prediction (test) mode\n",
    "\n",
    "    # 学習済みの重みをロード\n",
    "    epoch = 299\n",
    "    fname_weights = 'model_weights_%02d.h5'%(epoch)\n",
    "    fpath_weights = os.path.join(dname_checkpoints, fname_weights)\n",
    "    model.load_weights(fpath_weights)\n",
    "    print('==> done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading mean and standard deviation from stats01.npz...\n",
      "==> mean: [ 130.65464783   91.26850128   76.63642883]\n",
      "==> std : [ 55.28170013  43.99096298  43.11348343]\n",
      "==> done\n"
     ]
    }
   ],
   "source": [
    "    # Read Test Data\n",
    "    fnames = load_fnames('data/list_test_01.txt')\n",
    "\n",
    "    [fpaths_xs_test,fpaths_ys_test] = make_fnames(fnames,'data/img','data/mask','OperatorA_')\n",
    "\n",
    "    X_test = load_imgs_asarray(fpaths_xs_test, grayscale=False, target_size=target_size,\n",
    "                                dim_ordering=dim_ordering)\n",
    "    #Y_test = load_imgs_asarray(fpaths_ys_test, grayscale=True, target_size=target_size,\n",
    "    #                            dim_ordering=dim_ordering)\n",
    "\n",
    "    # トレーニング時に計算した平均・標準偏差をロード    \n",
    "    print('loading mean and standard deviation from ' + fname_stats + '...')\n",
    "    stats = np.load(fname_stats)\n",
    "    mean = stats['mean']\n",
    "    std = stats['std']\n",
    "    print('==> mean: ' + str(mean))\n",
    "    print('==> std : ' + str(std))\n",
    "\n",
    "    for i in range(3):\n",
    "        X_test[:, i] = (X_test[:, i] - mean[i]) / std[i]\n",
    "    print('==> done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # テストを開始\n",
    "    outputs = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving outputs as images...\n",
      "==> done\n"
     ]
    }
   ],
   "source": [
    "    # 出力を画像として保存\n",
    "    dname_outputs = './outputs/'\n",
    "    if not os.path.isdir(dname_outputs):\n",
    "        print('create directory: %s'%(dname_outputs))\n",
    "        os.mkdir(dname_outputs)\n",
    "\n",
    "    print('saving outputs as images...')\n",
    "    n = 0\n",
    "    for i, array in enumerate(outputs):\n",
    "        array = np.where(array > 0.5, 1, 0) # 二値に変換\n",
    "        array = array.astype(np.float32)\n",
    "        img_out = array_to_img(array, dim_ordering)\n",
    "        # fpath_out = os.path.join(dname_outputs, fnames[i])\n",
    "        fpath_out = os.path.join(dname_outputs, \"%05d.png\"%(n))\n",
    "        img_out.save(fpath_out)\n",
    "        n = n + 1\n",
    "\n",
    "    print('==> done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000: Dice Coeff = 0.867624\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-cd56b9785ba9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0moverlap_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim2a\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim3a\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%03d: Dice Coeff = %f'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moverlap_a\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moverlap_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%f'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mimg_dice_coeff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mim3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0mdice_eval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moverlap_a\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moverlap_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\pupil_segmenation\\image_ext.py\u001b[0m in \u001b[0;36mimg_dice_coeff\u001b[1;34m(im1, im2)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mimg_dice_coeff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mim2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# Compute dice coeff\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mim1a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mim1a\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mim1a\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mim2a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "    from PIL import Image\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    n = 0\n",
    "    dice_eval = []\n",
    "    \n",
    "    for i in range(len(fpaths_xs_test)):\n",
    "        # テスト画像\n",
    "        im1 = Image.open(fpaths_xs_test[i])\n",
    "        im1 = im1.resize((320,240)) \n",
    "        # 出力結果\n",
    "        im2 = Image.open(os.path.join(dname_outputs, \"%05d.png\"%(n)))\n",
    "        im2 = im2.resize((320,240))\n",
    "        # Grond Truth\n",
    "        im3 = Image.open(fpaths_ys_test[i])\n",
    "        im3 = im3.resize((320,240))\n",
    "\n",
    "        im2_d = np.zeros((240,320,3), 'uint8')\n",
    "        im2_d[:,:,0] = np.array(im2)\n",
    "        im2_d[:,:,1] = np.array(im3)*255\n",
    "        im2_d[:,:,2] = 0\n",
    "\n",
    "        # Compute dice coeff\n",
    "        im2a = np.array(im2)\n",
    "        im2a[im2a > 0] = 1\n",
    "        im3a = np.array(im3)\n",
    "        im3a[im3a > 0] = 1\n",
    "        \n",
    "        overlap_a = np.array(im2a) * np.array(im3a)\n",
    "        overlap_b = np.array(im2a) + np.array(im3a)\n",
    "        print('%03d: Dice Coeff = %f'%(i, 2*sum(sum(overlap_a))/sum(sum(overlap_b))))\n",
    "        print('%f'%img_dice_coeff(im2,im3))\n",
    "        dice_eval.append(2*sum(sum(overlap_a))/sum(sum(overlap_b)))\n",
    "\n",
    "        plt.imshow(np.hstack((np.array(im1),np.array(im2_d))))\n",
    "        plt.show()\n",
    "\n",
    "        n = n + 1\n",
    "    \n",
    "    print('Dice eval av. : %f'%np.mean(np.array(dice_eval)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   Show History\n",
    "#\n",
    "mode = \"SHOW_HISTORY\"\n",
    "if mode == \"SHOW_HISTORY\":\n",
    "    # load pickle\n",
    "    print(dname_checkpoints + '/' + fname_history)\n",
    "    history = pickle.load(open(dname_checkpoints + '/' + fname_history, 'rb'))\n",
    "    \n",
    "    for k in history.keys():\n",
    "        plt.plot(history[k])\n",
    "        plt.title(k)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_dice_coef', 'loss', 'dice_coef'])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
